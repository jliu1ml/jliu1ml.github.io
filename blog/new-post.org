#+TITLE:       Ideas and Math of Principal Component Analysis (PCA)
#+AUTHOR:      tp
#+EMAIL:       tp@ThinkPad
#+DATE:        2018-06-28 Thu
#+URI:         /blog/%y/%m/%d/new
#+KEYWORDS:    <TODO: insert your keywords here>
#+TAGS:        <TODO: insert your tags here>
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: <TODO: insert your description here>

#+STARTUP: inlineimages
#+STARTUP: content
#+STARTUP: latexpreview
#+OPTIONS: ^:nil
#+OPTIONS: num:nil


* 1. Purpose of PCA

Reduce data's feature dimension from *m* to *r*, so that different components are un-correlated.
For example, $[S,\sigma, \mu, -\mu, 2\sigma]^{T}\rightarrow [S,\sigma, \mu]^{T}$ reduces dimension from $m=5$ to $r=3$.

* 2. Set-up
- Suppose that our data set includes *n* data points. 
- We use a column vector with *m* entries to denote one data point with *m* features. Then the input of the PCA model is a $m\times n$ matrix *X*.
- The feature dimension of one output data point is *r*. So the output of the PCA model is a $r\times n$ matrix *Y*.

* 3. Idea flow
- The PCA process is a map $X\in\mathbb{R}^{m\times n}\rightarrow Y\in \mathbb{R}^{r\times n}$; the easiest choice is to use a linear map $P\in \mathbb{R}^{r\times m}$ to do the PCA through $PX=Y$.
- We want the features of $Y$ be uncorrelated:
  1. For convenience, we want each feature of $Y$ has zero mean, which could be done if each feature of $X$ has zero mean. In fact, 
    \begin{align*}
      \text{mean}(Y_{i\cdot})&=\frac{1}{n} \sum_{j=1}^n Y_{ij}=\frac{1}{n}
      \sum_{j=1}^n P_{i\cdot}X_{\cdot j}=\frac{1}{n} \sum_{j=1}^n
      \sum_{k=1}^m P_{ik} X_{kj}   \\ &= \sum_{k=1}^m P_{ik} \frac{1}{n}\sum_{j=1}^n  X_{kj}
      = \sum_{k=1}^m P_{ik} \text{mean}(X_{k\cdot}) =0.
    \end{align*}
    So at the first step of PCA, we need to make features of $X$ be of zero mean. From now on, we assume that features of both $X$ and $Y$ are of zero mean.
  2. Calculation of covariance matrix:
    \begin{align*}
      \text{COV}[X]=\frac{1}{n} XX^T,\quad \text{COV}[Y]=\frac{1}{n} YY^T.
    \end{align*}
    In fact,
    \begin{align*}
      \text{COV}[X_{i\cdot}, X_{i'\cdot}] & = \frac{1}{n} \sum_{j=1}^n
      \sum_{k=1}^m P_{ik} X_{kj} = (X_{ij}-\text{mean}(X_{i\cdot}))(X_{i'j}-\text{mean}(X_{i'\cdot}))\\
      & =\frac{1}{n} \sum_{j=1}^n X_{ij}X_{i'j}   = X_{i\cdot}(X^T)_{\cdot i'}= \frac{1}{n} (XX^T)_{ii'}.
    \end{align*}
  3. Relation between $\text{COV}[X]$ and $\text{COV}[Y]$:
    \begin{align*}
      \text{COV}[Y] = \frac{1}{n} YY^T = \frac{1}{n} PX (PX)^T 
      = \frac{1}{n} PXX^TP^T =  P \left( \frac{1}{n} XX^T \right) P^T = P \ \text{COV}[X] \ P^T
    \end{align*}
  4. To make features of $Y$ be uncorrelated, we just need to make $\text{COV}[Y]$ be diagonal. By equation the above equation, we further just need to find the $r\times m$ matrix $P$ so that $P \ \text{COV}[X] \ P^T$ is diagonal.
  5. Now, let's use the general method to diagonalize $\text{COV}[X]$. Since $\text{COV}[X]=\frac{1}{n} XX^T$, $\text{COV}[X]$ is a $m\times m$ real symmetric matrix. So there exist an orthonormal matrix $Q$, whose columns are formed by orthogonalized and normalized eigenvectors of $\text{COV}[X]$, such that
    \begin{align*}
      Q^T\text{COV}[X]Q=\text{diag} \left\{\lambda_1,\cdots,\lambda_{m}  \right\}
    \end{align*}
    where $Q_{\cdot i}$ is an eigenvector of $\lambda_{i}$.
    Take $P$ be the first $r$ rows of $Q^T$, then $P$ is a $r\times m$ matrix and 
    \begin{align*}
      \text{diag}\left\{ \lambda_1,\cdots,\lambda_m \right\}= Q^T
      \text{COV}[X] Q =
      \begin{bmatrix} P \\  P' \end{bmatrix} \text{COV}[X] \begin{bmatrix}
	P & P' \end{bmatrix} = \begin{bmatrix} P\ \text{COV}[X]P^T & P\
       \text{COV}[X] P'^T\\ P'\text{COV}[X]
      P^T & P' \text{COV}[X] P'^T  \end{bmatrix}
    \end{align*}
    So
    \begin{align*}
    COV[Y]= P\ \text{COV}[X]P^{T}=\text{diag} \left\{\lambda_1,\cdots,\lambda_{r} \right\}
    \end{align*}
- Furthermore, for each feature of $Y$, we want the variance over different data points be large, so that the data set contributes more information. By the above equation, we know that the variances of features of $Y$ are just eigenvalues $\lambda_{1},\cdots,\lambda_{r}$, so we just need to order the eigenvalues from larger ones to smaller ones when finding $Q$.

* 4. Algorithm of PCA
1. Format the data ($n$ data points with $m$ features) as a $m\times n$ matrix $X$.
2. For each row of $X$, subtract the row mean from each entry.
3. Calculate the covariance matrix $\text{COV}[X]=\frac{1}{n} XX^T$.
4. Calculate the eigenvalues and eigenvectors of $\text{COV}[X]$. Orthogonalize and normalize the eigenvectors; then use them to form the columns of the othornomal matrix $Q$ for diagonalizing $\text{COV}[X]$, with eigenvalues listed from larger ones to smaller ones.
5. Take the first $r$ rows of $Q^{T}$ to be $P$.
6. $Y=PX$ is the data we want after doing PCA.
 
* 5. Point of View from Linear Space

- From the point of view from linear space, the linear map $P=Q^T$ is just a change of basis. In fact, 
\begin{align*}
  Y&=PX=Q^T X= \begin{bmatrix} Q^T_{\cdot 1} \\ \vdots \\ Q^T_{\cdot
      r}  \end{bmatrix}_{r\times m} \begin{bmatrix} X_{\cdot 1}, &
    \cdots & X_{\cdot n} \end{bmatrix}_{m\times n} = \begin{bmatrix}
    \left\langle Q_{\cdot 1}, X_{\cdot 1}  \right\rangle  & \cdots &  \left\langle Q_{\cdot 1},
      X_{\cdot n}  \right\rangle \\ \vdots & \ddots & \vdots \\ \left\langle   Q_{\cdot r},
      X_{\cdot 1} \right\rangle & \cdots & \left\langle Q_{\cdot r},
      X_{\cdot n} \right\rangle\end{bmatrix}\\
  &= \begin{bmatrix}
  \frac{\left\langle Q_{\cdot 1}, X_{\cdot 1}
    \right\rangle}{\|Q_{\cdot 1}\|}     & \cdots &  \frac{\left\langle
      Q_{\cdot 1}, X_{\cdot n} \right\rangle}{\|Q_{\cdot 1}\|} \\
  \vdots &  \ddots & \vdots \\ \frac{\left\langle Q_{\cdot r}, X_{\cdot 1}
    \right\rangle}{\|Q_{\cdot r}\|} & \cdots & \frac{\left\langle
      Q_{\cdot r}, X_{\cdot n} \right\rangle}{\|Q_{\cdot
      r}\|} \end{bmatrix}
   = \begin{bmatrix}
     \text{Comp}_{Q_{\cdot 1}}X_{\cdot 1}   & \cdots &  \text{Comp}_{Q_{\cdot 1}}X_{\cdot n}  \\
  \vdots & \ddots  & \vdots \\ \text{Comp}_{Q_{\cdot r}}X_{\cdot 1}   & \cdots & \text{Comp}_{Q_{\cdot r}}X_{\cdot n}   \end{bmatrix}
\end{align*}
So every column of the output, i.e. every point in the reduced dimension, is
\begin{align*}
  Y_{\cdot j}=\begin{bmatrix} Y_{1 j} \\ \vdots \\
    Y_{rj} \end{bmatrix}
  =\begin{bmatrix} \text{Comp}_{Q_{\cdot 1}}X_{\cdot j}    \\
  \vdots  \\ \text{Comp}_{Q_{\cdot r}}X_{\cdot j}    \end{bmatrix}.
\end{align*}
Moreover,
\begin{align*}
  X_{\cdot j}=\begin{bmatrix} X_{1 j} \\ \vdots \\
    X_{rj} \end{bmatrix}
  =\begin{bmatrix} \text{Comp}_{e_{\cdot 1}}X_{\cdot j}    \\
  \vdots  \\ \text{Comp}_{e_{\cdot r}}X_{\cdot j}    \end{bmatrix}.
\end{align*}
So in general, $PX$ gives the coordinates of $X$ under the basis given by row-vectors of $P$.
Therefore, the output points are just coordinates of $X$ under the new basis formed by $r$ rows of $P$. The new basis, i.e. the $r$ rows of $P$, are orthonormal basis.
- In general, if we just need to reduce the dimension, the new basis need not be orthonormal. Besides for computation convenience, the main purpose to use orthonormal basis is because in Subsection 5 of Section 3, we need $Q^T$ other than $Q^{-1}$ for the diagonalization. 
- So using orthonormal basis is to ensure the linear independent of the output features. And we take larger eigenvalues is to make the distribution of the output points over each coordinate to be more sparse.




* Reference: 
http://blog.codinglabs.org/articles/pca-tutorial.html


