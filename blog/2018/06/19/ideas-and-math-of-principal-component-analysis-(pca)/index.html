<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Ideas and Math of Principal Component Analysis (PCA) - org-page</title>
    <meta charset="utf-8" />
    <meta name="author" content="tp" />
    <link rel="stylesheet" href="/media/css/main.css" type="text/css">
    <link rel="stylesheet" href="/media/css/prettify.css" type="text/css">
  </head>
  <body class="container">
    <div>
      <header class="masthead">
        <h1 class="masthead-title"><a href="/">org-page</a></h1>
        <p>static site generator</p>
        <ul>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/tags/">Tags</a></li>
          <li><a href="/about/">About</a></li>
          <li><a href="https://github.com/jliu1ml">GitHub</a></li>
          <li><a href="/rss.xml">RSS</a></li>
        </ul>
        <form method="get" id="searchform" action="//www.google.com/search">
          <input type="text" class="field" name="q" id="s" placeholder="Search">
          <input type="hidden" name="as_sitesearch" value="jliu1ml.github.io">
        </form>
      </header>
    </div>

<div>
<div class="post">
<h1>Ideas and Math of Principal Component Analysis (PCA)</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org305d69a">1. Purpose of PCA</a></li>
<li><a href="#org3292ec8">2. Set-up</a></li>
<li><a href="#orgb7a9ae3">3. Idea flow</a></li>
<li><a href="#org28d3d8b">4. Algorithm of PCA</a></li>
<li><a href="#orgfa97fad">5. Point of View from Linear Space</a></li>
<li><a href="#org01cbc92">Reference:</a></li>
</ul>
</div>
</div>
<p>
;#+OPTIONS: tex:dvisvgm
</p>

<div id="outline-container-org305d69a" class="outline-2">
<h2 id="org305d69a">1. Purpose of PCA</h2>
<div class="outline-text-2" id="text-org305d69a">
<p>
Reduce data's feature dimension from <b>m</b> to <b>r</b>, so that different components are un-correlated.
For example, <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_af3593fe77a4545b6c0d50e058f56da82d7e2f46.png" alt="PCA_af3593fe77a4545b6c0d50e058f56da82d7e2f46.png" /> reduces dimension from <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_47514aaa898e0e95ac079a973bc6afd41aade2fe.png" alt="PCA_47514aaa898e0e95ac079a973bc6afd41aade2fe.png" /> to <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_74c7eb266524c8c571dd0bcb279c51350bd40395.png" alt="PCA_74c7eb266524c8c571dd0bcb279c51350bd40395.png" />.
</p>
</div>
</div>

<div id="outline-container-org3292ec8" class="outline-2">
<h2 id="org3292ec8">2. Set-up</h2>
<div class="outline-text-2" id="text-org3292ec8">
<ul class="org-ul">
<li>Suppose that our data set includes <b>n</b> data points.</li>
<li>We use a column vector with <b>m</b> entries to denote one data point with <b>m</b> features. Then the input of the PCA model is a <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_4f986b1319986be8c3f492bc31ad6a22cd931634.png" alt="PCA_4f986b1319986be8c3f492bc31ad6a22cd931634.png" /> matrix <b>X</b>.</li>
<li>The feature dimension of one output data point is <b>r</b>. So the output of the PCA model is a <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_421f46925f0f3c3702412c4bfe03327bed43257d.png" alt="PCA_421f46925f0f3c3702412c4bfe03327bed43257d.png" /> matrix <b>Y</b>.</li>
</ul>
</div>
</div>

<div id="outline-container-orgb7a9ae3" class="outline-2">
<h2 id="orgb7a9ae3">3. Idea flow</h2>
<div class="outline-text-2" id="text-orgb7a9ae3">
<ul class="org-ul">
<li>The PCA process is a map <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_db7a04bfe64ae788bef208edc03613cef0c7a812.png" alt="PCA_db7a04bfe64ae788bef208edc03613cef0c7a812.png" />; the easiest choice is to use a linear map <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_a5d8ca44326aa01c46a1be7cb0791d78d51f0bf4.png" alt="PCA_a5d8ca44326aa01c46a1be7cb0791d78d51f0bf4.png" /> to do the PCA through <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_e6834752611e16e6c3061e7ad978c654bd8c11f9.png" alt="PCA_e6834752611e16e6c3061e7ad978c654bd8c11f9.png" />.</li>
<li>We want the features of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" alt="PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" /> be uncorrelated:
<ol class="org-ol">
<li><p>
For convenience, we want each feature of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" alt="PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" /> has zero mean, which could be done if each feature of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" alt="PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" /> has zero mean. In fact, 
</p>

<div class="figure">
<p><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_0c1819f67067a8a0ba6999ec27a56666122a913b.png" alt="PCA_0c1819f67067a8a0ba6999ec27a56666122a913b.png" /></p>
</div>
<p>
So at the first step of PCA, we need to make features of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" alt="PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" /> be of zero mean. From now on, we assume that features of both <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" alt="PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" /> and <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" alt="PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" /> are of zero mean.
</p></li>
<li><p>
Calculation of covariance matrix:
</p>

<div class="figure">
<p><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_ba49f5021e647d976677efa27605e3ac17910c86.png" alt="PCA_ba49f5021e647d976677efa27605e3ac17910c86.png" /></p>
</div>
<p>
In fact,
</p>

<div class="figure">
<p><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_3d422d6be1e7197ccc9283d83a35ba1755c0a0f1.png" alt="PCA_3d422d6be1e7197ccc9283d83a35ba1755c0a0f1.png" /></p>
</div></li>
<li><p>
Relation between <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" alt="PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" /> and <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_893b7567d52c1167e03e3a128c4f03f06c26eab1.png" alt="PCA_893b7567d52c1167e03e3a128c4f03f06c26eab1.png" />:
</p>

<div class="figure">
<p><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_a113e4b1df19d6f8ca07f03cd8c44e337bb52d23.png" alt="PCA_a113e4b1df19d6f8ca07f03cd8c44e337bb52d23.png" /></p>
</div></li>
<li>To make features of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" alt="PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" /> be uncorrelated, we just need to make <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_893b7567d52c1167e03e3a128c4f03f06c26eab1.png" alt="PCA_893b7567d52c1167e03e3a128c4f03f06c26eab1.png" /> be diagonal. By equation the above equation, we further just need to find the <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_146393e1d635661ee06adfdbc8a5c446a61b7ef0.png" alt="PCA_146393e1d635661ee06adfdbc8a5c446a61b7ef0.png" /> matrix <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" alt="PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" /> so that <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_bbf49b6ff5952b4aaf2071b00464a2fee931890d.png" alt="PCA_bbf49b6ff5952b4aaf2071b00464a2fee931890d.png" /> is diagonal.</li>
<li><p>
Now, let's use the general method to diagonalize <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" alt="PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" />. Since <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_4b7c27871e43805f9f9eef4ecd136b25edf58b5c.png" alt="PCA_4b7c27871e43805f9f9eef4ecd136b25edf58b5c.png" />, <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" alt="PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" /> is a <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_44a150613850ba11952059fbd36053d722016ff3.png" alt="PCA_44a150613850ba11952059fbd36053d722016ff3.png" /> real symmetric matrix. So there exist an orthonormal matrix <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_71b3ece4696c590c73b04085654e973ff929ec5c.png" alt="PCA_71b3ece4696c590c73b04085654e973ff929ec5c.png" />, whose columns are formed by orthogonalized and normalized eigenvectors of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" alt="PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" />, such that
</p>

<div class="figure">
<p><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_28c6237503bbe23d381f98e2ce0b59cdea0d9492.png" alt="PCA_28c6237503bbe23d381f98e2ce0b59cdea0d9492.png" /></p>
</div>
<p>
where <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_14a9a18ef9051afb5a2c2783eb44d04b98ac3217.png" alt="PCA_14a9a18ef9051afb5a2c2783eb44d04b98ac3217.png" /> is an eigenvector of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_5beb6c02ce1a823f7061021182bd062d0fd0bb3e.png" alt="PCA_5beb6c02ce1a823f7061021182bd062d0fd0bb3e.png" />.
Take <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" alt="PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" /> be the first <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_2fe846a658f432822b9de390b859847af97dc794.png" alt="PCA_2fe846a658f432822b9de390b859847af97dc794.png" /> rows of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_ebbb18d2a45b9c65fbf38062f02d6abb36416e4d.png" alt="PCA_ebbb18d2a45b9c65fbf38062f02d6abb36416e4d.png" />, then <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" alt="PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" /> is a <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_146393e1d635661ee06adfdbc8a5c446a61b7ef0.png" alt="PCA_146393e1d635661ee06adfdbc8a5c446a61b7ef0.png" /> matrix and 
</p>

<div class="figure">
<p><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_b8da084d856322c7d5af22dc94b5e9d009ddac8b.png" alt="PCA_b8da084d856322c7d5af22dc94b5e9d009ddac8b.png" /></p>
</div>
<p>
So
</p>

<div class="figure">
<p><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_f445debad8a8f4dc9e68a616145c32777f3dc914.png" alt="PCA_f445debad8a8f4dc9e68a616145c32777f3dc914.png" /></p>
</div></li>
</ol></li>
<li>Furthermore, for each feature of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" alt="PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" />, we want the variance over different data points be large, so that the data set contributes more information. By the above equation, we know that the variances of features of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" alt="PCA_fc6df8c6fae22a631cc61960a31416bc9f27427d.png" /> are just eigenvalues <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_304bc3a7d76e6d8bd58e9a4cc9aed3bcceeb1da6.png" alt="PCA_304bc3a7d76e6d8bd58e9a4cc9aed3bcceeb1da6.png" />, so we just need to order the eigenvalues from larger ones to smaller ones when finding <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_71b3ece4696c590c73b04085654e973ff929ec5c.png" alt="PCA_71b3ece4696c590c73b04085654e973ff929ec5c.png" />.</li>
</ul>
</div>
</div>

<div id="outline-container-org28d3d8b" class="outline-2">
<h2 id="org28d3d8b">4. Algorithm of PCA</h2>
<div class="outline-text-2" id="text-org28d3d8b">
<ol class="org-ol">
<li>Format the data (<img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_73c82416c200ab8e29ab2e7b01a14a00412caec3.png" alt="PCA_73c82416c200ab8e29ab2e7b01a14a00412caec3.png" /> data points with <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_c638e910e470631c4499036e7a1a2db88ef8da9b.png" alt="PCA_c638e910e470631c4499036e7a1a2db88ef8da9b.png" /> features) as a <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_4f986b1319986be8c3f492bc31ad6a22cd931634.png" alt="PCA_4f986b1319986be8c3f492bc31ad6a22cd931634.png" /> matrix <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" alt="PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" />.</li>
<li>For each row of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" alt="PCA_6d4f5eecbb47d716c048e5552508ef6112a8f19a.png" />, substract the row mean from each entry.</li>
<li>Calculate the covariance matrix <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_4b7c27871e43805f9f9eef4ecd136b25edf58b5c.png" alt="PCA_4b7c27871e43805f9f9eef4ecd136b25edf58b5c.png" />.</li>
<li>Calculate the eigenvalues and eigenvectors of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" alt="PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" />. Orthogonalize and normalize the eigenvectors; then use them to form the columns of the othornomal matrix <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_71b3ece4696c590c73b04085654e973ff929ec5c.png" alt="PCA_71b3ece4696c590c73b04085654e973ff929ec5c.png" /> for diagonalizing <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" alt="PCA_c84f82141272671487d8824b7f6a3ce6c2633069.png" />, with eigenvalues listed from larger ones to smaller ones.</li>
<li>Take the first <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_2fe846a658f432822b9de390b859847af97dc794.png" alt="PCA_2fe846a658f432822b9de390b859847af97dc794.png" /> rows of <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_b7823d5a0d35690d2ef51cf3cb54c310fde6a8c7.png" alt="PCA_b7823d5a0d35690d2ef51cf3cb54c310fde6a8c7.png" /> to be <img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" alt="PCA_fb8ab218a16ecee7f0429791c324e05a06904834.png" />.</li>
<li><img src="/assets/blog/2018/06/19/ideas-and-math-of-principal-component-analysis-(pca)/PCA_de3d077b4ff213c186f1eca3ee88b014321ddad0.png" alt="PCA_de3d077b4ff213c186f1eca3ee88b014321ddad0.png" /> is the data we want after doing PCA.</li>
</ol>
</div>
</div>

<div id="outline-container-orgfa97fad" class="outline-2">
<h2 id="orgfa97fad">5. Point of View from Linear Space</h2>
</div>


<div id="outline-container-org01cbc92" class="outline-2">
<h2 id="org01cbc92">Reference:</h2>
<div class="outline-text-2" id="text-org01cbc92">
<p>
<a href="http://blog.codinglabs.org/articles/pca-tutorial.html">http://blog.codinglabs.org/articles/pca-tutorial.html</a>
</p>
</div>
</div>

</div>
</div>
    <div>
      <div class="post-meta">
        <span title="post date" class="post-info">2018-06-19</span>
        <span title="last modification date" class="post-info">2018-06-19</span>
        <span title="tags" class="post-info">N/A</span>
        <span title="author" class="post-info">tp</span>
      </div>
      <section>
        <h1>Comments</h1>
      </section>
      <script src="//code.jquery.com/jquery-latest.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="/media/js/main.js"></script>
      <div class="footer">
        <p>Generated by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.x (<a href="http://orgmode.org">Org mode</a> 9.x)</p>
        <p>
          Copyright &copy; 2012 - <span id="footerYear"></span> <a href="mailto:tp &lt;at&gt; ThinkPad">tp</a>
          &nbsp;&nbsp;-&nbsp;&nbsp;
          Powered by <a href="https://github.com/kelvinh/org-page" target="_blank">org-page</a>
          <script type="text/javascript">document.getElementById("footerYear").innerHTML = (new Date()).getFullYear();</script>
        </p>
      </div>
    </div>

  </body>
</html>
