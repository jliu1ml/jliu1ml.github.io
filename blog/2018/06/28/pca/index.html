<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Ideas and Math of Principal Component Analysis (PCA) - org-page</title>
    <meta charset="utf-8" />
    <meta name="author" content="tp" />
    <meta name="description" content="&lt;TODO: insert your description here&gt;" />
    <meta name="keywords" content="&lt;TODO: insert your keywords here&gt;" />
    <link rel="stylesheet" href="/media/css/main.css" type="text/css">
    <link rel="stylesheet" href="/media/css/prettify.css" type="text/css">
  </head>
  <body class="container">
    <div>
      <header class="masthead">
        <h1 class="masthead-title"><a href="/">org-page</a></h1>
        <p>static site generator</p>
        <ul>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/tags/">Tags</a></li>
          <li><a href="/about/">About</a></li>
          <li><a href="https://github.com/kelvinh/org-page">GitHub</a></li>
          <li><a href="/rss.xml">RSS</a></li>
        </ul>
        <form method="get" id="searchform" action="//www.google.com/search">
          <input type="text" class="field" name="q" id="s" placeholder="Search">
          <input type="hidden" name="as_sitesearch" value="jliu1ml.github.io">
        </form>
      </header>
    </div>

<div>
<div class="post">
<h1>Ideas and Math of Principal Component Analysis (PCA)</h1>

<div id="outline-container-org0fb4a2f" class="outline-2">
<h2 id="org0fb4a2f">1. Purpose of PCA</h2>
<div class="outline-text-2" id="text-org0fb4a2f">
<p>
Reduce data's feature dimension from <b>m</b> to <b>r</b>, so that different components are un-correlated.
For example, \([S,\sigma, \mu, -\mu, 2\sigma]^{T}\rightarrow [S,\sigma, \mu]^{T}\) reduces dimension from \(m=5\) to \(r=3\).
</p>
</div>
</div>

<div id="outline-container-org03bf249" class="outline-2">
<h2 id="org03bf249">2. Set-up</h2>
<div class="outline-text-2" id="text-org03bf249">
<ul class="org-ul">
<li>Suppose that our data set includes <b>n</b> data points.</li>
<li>We use a column vector with <b>m</b> entries to denote one data point with <b>m</b> features. Then the input of the PCA model is a \(m\times n\) matrix <b>X</b>.</li>
<li>The feature dimension of one output data point is <b>r</b>. So the output of the PCA model is a \(r\times n\) matrix <b>Y</b>.</li>
</ul>
</div>
</div>

<div id="outline-container-orgdb6979b" class="outline-2">
<h2 id="orgdb6979b">3. Idea flow</h2>
<div class="outline-text-2" id="text-orgdb6979b">
<ul class="org-ul">
<li>The PCA process is a map \(X\in\mathbb{R}^{m\times n}\rightarrow Y\in \mathbb{R}^{r\times n}\); the easiest choice is to use a linear map \(P\in \mathbb{R}^{r\times m}\) to do the PCA through \(PX=Y\).</li>
<li>We want the features of \(Y\) be uncorrelated:
<ol class="org-ol">
<li><p>
For convenience, we want each feature of \(Y\) has zero mean, which could be done if each feature of \(X\) has zero mean. In fact, 
</p>
\begin{align*}
  \text{mean}(Y_{i\cdot})&=\frac{1}{n} \sum_{j=1}^n Y_{ij}=\frac{1}{n}
  \sum_{j=1}^n P_{i\cdot}X_{\cdot j}=\frac{1}{n} \sum_{j=1}^n
  \sum_{k=1}^m P_{ik} X_{kj}   \\ &= \sum_{k=1}^m P_{ik} \frac{1}{n}\sum_{j=1}^n  X_{kj}
  = \sum_{k=1}^m P_{ik} \text{mean}(X_{k\cdot}) =0.
\end{align*}
<p>
So at the first step of PCA, we need to make features of \(X\) be of zero mean. From now on, we assume that features of both \(X\) and \(Y\) are of zero mean.
</p></li>
<li><p>
Calculation of covariance matrix:
</p>
\begin{align*}
  \text{COV}[X]=\frac{1}{n} XX^T,\quad \text{COV}[Y]=\frac{1}{n} YY^T.
\end{align*}
<p>
In fact,
</p>
\begin{align*}
  \text{COV}[X_{i\cdot}, X_{i'\cdot}] & = \frac{1}{n} \sum_{j=1}^n
  \sum_{k=1}^m P_{ik} X_{kj} = (X_{ij}-\text{mean}(X_{i\cdot}))(X_{i'j}-\text{mean}(X_{i'\cdot}))\\
  & =\frac{1}{n} \sum_{j=1}^n X_{ij}X_{i'j}   = X_{i\cdot}(X^T)_{\cdot i'}= \frac{1}{n} (XX^T)_{ii'}.
\end{align*}</li>
<li><p>
Relation between \(\text{COV}[X]\) and \(\text{COV}[Y]\):
</p>
\begin{align*}
  \text{COV}[Y] = \frac{1}{n} YY^T = \frac{1}{n} PX (PX)^T 
  = \frac{1}{n} PXX^TP^T =  P \left( \frac{1}{n} XX^T \right) P^T = P \ \text{COV}[X] \ P^T
\end{align*}</li>
<li>To make features of \(Y\) be uncorrelated, we just need to make \(\text{COV}[Y]\) be diagonal. By equation the above equation, we further just need to find the \(r\times m\) matrix \(P\) so that \(P \ \text{COV}[X] \ P^T\) is diagonal.</li>
<li><p>
Now, let's use the general method to diagonalize \(\text{COV}[X]\). Since \(\text{COV}[X]=\frac{1}{n} XX^T\), \(\text{COV}[X]\) is a \(m\times m\) real symmetric matrix. So there exist an orthonormal matrix \(Q\), whose columns are formed by orthogonalized and normalized eigenvectors of \(\text{COV}[X]\), such that
</p>
\begin{align*}
  Q^T\text{COV}[X]Q=\text{diag} \left\{\lambda_1,\cdots,\lambda_{m}  \right\}
\end{align*}
<p>
where \(Q_{\cdot i}\) is an eigenvector of \(\lambda_{i}\).
Take \(P\) be the first \(r\) rows of \(Q^T\), then \(P\) is a \(r\times m\) matrix and 
</p>
\begin{align*}
  \text{diag}\left\{ \lambda_1,\cdots,\lambda_m \right\}= Q^T
  \text{COV}[X] Q =
  \begin{bmatrix} P \\  P' \end{bmatrix} \text{COV}[X] \begin{bmatrix}
    P & P' \end{bmatrix} = \begin{bmatrix} P\ \text{COV}[X]P^T & P\
   \text{COV}[X] P'^T\\ P'\text{COV}[X]
  P^T & P' \text{COV}[X] P'^T  \end{bmatrix}
\end{align*}
<p>
So
</p>
\begin{align*}
COV[Y]= P\ \text{COV}[X]P^{T}=\text{diag} \left\{\lambda_1,\cdots,\lambda_{r} \right\}
\end{align*}</li>
</ol></li>
<li>Furthermore, for each feature of \(Y\), we want the variance over different data points be large, so that the data set contributes more information. By the above equation, we know that the variances of features of \(Y\) are just eigenvalues \(\lambda_{1},\cdots,\lambda_{r}\), so we just need to order the eigenvalues from larger ones to smaller ones when finding \(Q\).</li>
</ul>
</div>
</div>

<div id="outline-container-org6eecd09" class="outline-2">
<h2 id="org6eecd09">4. Algorithm of PCA</h2>
<div class="outline-text-2" id="text-org6eecd09">
<ol class="org-ol">
<li>Format the data (\(n\) data points with \(m\) features) as a \(m\times n\) matrix \(X\).</li>
<li>For each row of \(X\), subtract the row mean from each entry.</li>
<li>Calculate the covariance matrix \(\text{COV}[X]=\frac{1}{n} XX^T\).</li>
<li>Calculate the eigenvalues and eigenvectors of \(\text{COV}[X]\). Orthogonalize and normalize the eigenvectors; then use them to form the columns of the othornomal matrix \(Q\) for diagonalizing \(\text{COV}[X]\), with eigenvalues listed from larger ones to smaller ones.</li>
<li>Take the first \(r\) rows of \(Q^{T}\) to be \(P\).</li>
<li>\(Y=PX\) is the data we want after doing PCA.</li>
</ol>
</div>
</div>

<div id="outline-container-org6dec060" class="outline-2">
<h2 id="org6dec060">5. Point of View from Linear Space</h2>
<div class="outline-text-2" id="text-org6dec060">
<ul class="org-ul">
<li>From the point of view from linear space, the linear map \(P=Q^T\) is just a change of basis. In fact,</li>
</ul>
\begin{align*}
  Y&=PX=Q^T X= \begin{bmatrix} Q^T_{\cdot 1} \\ \vdots \\ Q^T_{\cdot
      r}  \end{bmatrix}_{r\times m} \begin{bmatrix} X_{\cdot 1}, &
    \cdots & X_{\cdot n} \end{bmatrix}_{m\times n} = \begin{bmatrix}
    \left\langle Q_{\cdot 1}, X_{\cdot 1}  \right\rangle  & \cdots &  \left\langle Q_{\cdot 1},
      X_{\cdot n}  \right\rangle \\ \vdots & \ddots & \vdots \\ \left\langle   Q_{\cdot r},
      X_{\cdot 1} \right\rangle & \cdots & \left\langle Q_{\cdot r},
      X_{\cdot n} \right\rangle\end{bmatrix}\\
  &= \begin{bmatrix}
  \frac{\left\langle Q_{\cdot 1}, X_{\cdot 1}
    \right\rangle}{\|Q_{\cdot 1}\|}     & \cdots &  \frac{\left\langle
      Q_{\cdot 1}, X_{\cdot n} \right\rangle}{\|Q_{\cdot 1}\|} \\
  \vdots &  \ddots & \vdots \\ \frac{\left\langle Q_{\cdot r}, X_{\cdot 1}
    \right\rangle}{\|Q_{\cdot r}\|} & \cdots & \frac{\left\langle
      Q_{\cdot r}, X_{\cdot n} \right\rangle}{\|Q_{\cdot
      r}\|} \end{bmatrix}
   = \begin{bmatrix}
     \text{Comp}_{Q_{\cdot 1}}X_{\cdot 1}   & \cdots &  \text{Comp}_{Q_{\cdot 1}}X_{\cdot n}  \\
  \vdots & \ddots  & \vdots \\ \text{Comp}_{Q_{\cdot r}}X_{\cdot 1}   & \cdots & \text{Comp}_{Q_{\cdot r}}X_{\cdot n}   \end{bmatrix}
\end{align*}
<p>
So every column of the output, i.e. every point in the reduced dimension, is
</p>
\begin{align*}
  Y_{\cdot j}=\begin{bmatrix} Y_{1 j} \\ \vdots \\
    Y_{rj} \end{bmatrix}
  =\begin{bmatrix} \text{Comp}_{Q_{\cdot 1}}X_{\cdot j}    \\
  \vdots  \\ \text{Comp}_{Q_{\cdot r}}X_{\cdot j}    \end{bmatrix}.
\end{align*}
<p>
Moreover,
</p>
\begin{align*}
  X_{\cdot j}=\begin{bmatrix} X_{1 j} \\ \vdots \\
    X_{rj} \end{bmatrix}
  =\begin{bmatrix} \text{Comp}_{e_{\cdot 1}}X_{\cdot j}    \\
  \vdots  \\ \text{Comp}_{e_{\cdot r}}X_{\cdot j}    \end{bmatrix}.
\end{align*}
<p>
So in general, \(PX\) gives the coordinates of \(X\) under the basis given by row-vectors of \(P\).
Therefore, the output points are just coordinates of \(X\) under the new basis formed by \(r\) rows of \(P\). The new basis, i.e. the \(r\) rows of \(P\), are orthonormal basis.
</p>
<ul class="org-ul">
<li>In general, if we just need to reduce the dimension, the new basis need not be orthonormal. Besides for computation convenience, the main purpose to use orthonormal basis is because in Subsection 5 of Section 3, we need \(Q^T\) other than \(Q^{-1}\) for the diagonalization.</li>
<li>So using orthonormal basis is to ensure the linear independent of the output features. And we take larger eigenvalues is to make the distribution of the output points over each coordinate to be more sparse.</li>
</ul>
</div>
</div>




<div id="outline-container-orgd3955c5" class="outline-2">
<h2 id="orgd3955c5">Reference:</h2>
<div class="outline-text-2" id="text-orgd3955c5">
<p>
<a href="http://blog.codinglabs.org/articles/pca-tutorial.html">http://blog.codinglabs.org/articles/pca-tutorial.html</a>
</p>
</div>
</div>

</div>
</div>
    <div>
      <div class="post-meta">
        <span title="post date" class="post-info">2018-06-28</span>
        <span title="last modification date" class="post-info">2018-06-28</span>
        <span title="tags" class="post-info"><a href="/tags/&lt;todo/">&lt;TODO</a>, <a href="/tags/insert-your-tags-here&gt;/">insert your tags here&gt;</a></span>
        <span title="author" class="post-info">tp</span>
      </div>
      <section>
        <h1>Comments</h1>
      </section>
      <script src="//code.jquery.com/jquery-latest.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="/media/js/main.js"></script>
      <div class="footer">
        <p>Generated by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.x (<a href="http://orgmode.org">Org mode</a> 9.x)</p>
        <p>
          Copyright &copy; 2012 - <span id="footerYear"></span> <a href="mailto:tp &lt;at&gt; ThinkPad">tp</a>
          &nbsp;&nbsp;-&nbsp;&nbsp;
          Powered by <a href="https://github.com/kelvinh/org-page" target="_blank">org-page</a>
          <script type="text/javascript">document.getElementById("footerYear").innerHTML = (new Date()).getFullYear();</script>
        </p>
      </div>
    </div>

  </body>
</html>
